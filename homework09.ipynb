{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework09.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsyY9G-HI_qD"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tensorflow.keras import Model\r\n",
        "from random import randint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaJbcFszkFJu"
      },
      "source": [
        "def generate_sequence(num):\r\n",
        "  \"\"\"\r\n",
        "  Generates a random sequence of digits from 0-9 of a given length.\r\n",
        "  num:        Number of digits to be generated\r\n",
        "  \"\"\"\r\n",
        "  return tf.random.uniform(shape=(num,1), maxval=9, dtype=tf.dtypes.int32)\r\n",
        "\r\n",
        "def more_common(sequence, queryA, queryB):\r\n",
        "  \"\"\"\r\n",
        "  Checks which of two given digits occur more often in a given sequence.\r\n",
        "  sequence:   (list) Sequence of single digits\r\n",
        "  queryA:     (int) First digit to check number of occurences for (range 0-9)\r\n",
        "  queryB:     (int) Second digit to check number of occurences for (range 0-9)\r\n",
        "  Return:     Value that occurs more often or 'equal' if both queries occur \r\n",
        "              equally often\r\n",
        "  \"\"\"\r\n",
        "  vals, _, counts = tf.unique_with_counts(tf.reshape(sequence, shape=(-1,)))\r\n",
        "\r\n",
        "  count_queryA = np.array(counts)[np.argwhere(np.array(vals) == queryA)] if queryA in np.array(vals) else 0\r\n",
        "  count_queryB = np.array(counts)[np.argwhere(np.array(vals) == queryB)] if queryB in np.array(vals) else 0\r\n",
        "\r\n",
        "  # Code as... \r\n",
        "  # 0 -> both occur equally often, \r\n",
        "  # 1 -> first query occurs more often\r\n",
        "  # 2 -> second query occurs more often:\r\n",
        "  if count_queryA > count_queryB:\r\n",
        "    return tf.convert_to_tensor([1])\r\n",
        "  elif count_queryB > count_queryA:\r\n",
        "    return tf.convert_to_tensor([2])\r\n",
        "  else:\r\n",
        "    # they occur equally often\r\n",
        "    return tf.convert_to_tensor([0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkUxotRiZ458"
      },
      "source": [
        "**Functionality test for sequence generation and detecting more common value given two digits.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hppHUUDXJHGZ",
        "outputId": "999c2669-c966-4bc7-894e-9a764a18cd67"
      },
      "source": [
        "seq = generate_sequence(10)\r\n",
        "seq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 1), dtype=int32, numpy=\n",
              "array([[8],\n",
              "       [3],\n",
              "       [6],\n",
              "       [4],\n",
              "       [0],\n",
              "       [3],\n",
              "       [3],\n",
              "       [8],\n",
              "       [6],\n",
              "       [3]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlFreuyLJMHg",
        "outputId": "1cc2a772-8fff-4ac2-8f23-723f4951e918"
      },
      "source": [
        "result = more_common(seq, 9, 1)\r\n",
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEWhe6KUjH-l"
      },
      "source": [
        "## Create Dataset\r\n",
        "\r\n",
        "Each dataset entry consists of a sequence of a given length and a query about two numbers. The label describes, whether the queried numbers occur equally often (label '0'), or which of the two numbers occur more often (label '1' for the first query, label '2' for the second query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xasIUCPQZIoC"
      },
      "source": [
        "BATCH_SIZE = 10\r\n",
        "TRAIN_DATASET_LENGTH = 100\r\n",
        "TEST_DATASET_LENGTH = 50\r\n",
        "SEQUENCE_LENGTH = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSwlaY0TjGxN"
      },
      "source": [
        "# Number of data sequence - target pairs\r\n",
        "complete_dataset_length = TRAIN_DATASET_LENGTH + TEST_DATASET_LENGTH\r\n",
        "\r\n",
        "# Create targets (two random numbers)\r\n",
        "targets = [generate_sequence(2) for _ in range(complete_dataset_length)]\r\n",
        "\r\n",
        "# Create sequences of length 25\r\n",
        "sequences = [generate_sequence(SEQUENCE_LENGTH) for _ in range(complete_dataset_length)]\r\n",
        "\r\n",
        "# Create labels (as above, )\r\n",
        "labels = [more_common(seq, target[0], target[1]) for seq, target in zip(sequences, targets)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXryWV_Jlod6"
      },
      "source": [
        "Now we want to create a tensorflow dataset from that. We preposess all sequence, target, label as one hot encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iTDesAjl1vi",
        "outputId": "8623528e-7380-4a5f-fdb5-5889a200c6ea"
      },
      "source": [
        "# Create dataset\r\n",
        "train_data = tf.data.Dataset.from_tensor_slices((targets[:TRAIN_DATASET_LENGTH], sequences[:TRAIN_DATASET_LENGTH], labels[:TRAIN_DATASET_LENGTH]))\r\n",
        "test_data = tf.data.Dataset.from_tensor_slices((targets[TRAIN_DATASET_LENGTH:], sequences[TRAIN_DATASET_LENGTH:], labels[TRAIN_DATASET_LENGTH:]))\r\n",
        "\r\n",
        "# Taking a look at the data\r\n",
        "for data in train_data.take(1):\r\n",
        "  print('Train data example:\\n Target: %s\\n Sequence: %s\\n Label: %s\\n' % (data[:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data example:\n",
            " Target: tf.Tensor(\n",
            "[[0]\n",
            " [7]], shape=(2, 1), dtype=int32)\n",
            " Sequence: tf.Tensor(\n",
            "[[7]\n",
            " [3]\n",
            " [3]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [4]\n",
            " [5]\n",
            " [7]\n",
            " [3]], shape=(10, 1), dtype=int32)\n",
            " Label: tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXK6OvQrkAKZ",
        "outputId": "9acf2f1f-ff90-4bc1-90a2-cd66213d1805"
      },
      "source": [
        "# PREPROCESSING:\r\n",
        "\r\n",
        "# One hot encoding\r\n",
        "# t -> target, s -> sequence, l -> label\r\n",
        "train_data_prep = train_data.map(lambda t, s, l: (tf.one_hot(t, 10), tf.one_hot(s, 10), tf.one_hot(l, 3)))\r\n",
        "test_data_prep = test_data.map(lambda t, s, l: (tf.one_hot(t, 10), tf.one_hot(s, 10), tf.one_hot(l, 3)))\r\n",
        "\r\n",
        "# Print one example after one hot encoding\r\n",
        "for data in train_data_prep.take(1):\r\n",
        "  print('Train data example:\\n\\n Target: %s\\n\\n Sequence: %s\\n\\n Label: %s\\n' % (data[:]))\r\n",
        "\r\n",
        "# Batching\r\n",
        "train_data_prep = train_data_prep.batch(BATCH_SIZE)\r\n",
        "test_data_prep = test_data_prep.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data example:\n",
            "\n",
            " Target: tf.Tensor(\n",
            "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]], shape=(2, 1, 10), dtype=float32)\n",
            "\n",
            " Sequence: tf.Tensor(\n",
            "[[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]], shape=(10, 1, 10), dtype=float32)\n",
            "\n",
            " Label: tf.Tensor([[0. 0. 1.]], shape=(1, 3), dtype=float32)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrQvNqgQavE4",
        "outputId": "3c1b7abd-bccd-40d8-c85b-e92eb84bcc61"
      },
      "source": [
        "print(train_data_prep)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((None, 2, 1, 10), (None, 10, 1, 10), (None, 1, 3)), types: (tf.float32, tf.float32, tf.float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZh3ujDeMTUw"
      },
      "source": [
        "## LSTM Cell\r\n",
        "\r\n",
        "A LSTM cell consists of:\r\n",
        "* a cell state\r\n",
        "* three gates (input, forget, output)\r\n",
        "* a cell state candidate\r\n",
        "* a hidden state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh-SIbsGSVHz"
      },
      "source": [
        "# Activation functions\r\n",
        "SIGMOID = tf.keras.activations.sigmoid\r\n",
        "TANH = tf.keras.activations.tanh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD5gNtquKUtK"
      },
      "source": [
        "class LSTM_Cell(Model):\r\n",
        "\r\n",
        "  def __init__(self, units):\r\n",
        "    \"\"\"\r\n",
        "    \"\"\"\r\n",
        "    super(LSTM_Cell, self).__init__()\r\n",
        "\r\n",
        "    # orthogonale kernel initializer nutzen\r\n",
        "\r\n",
        "    self.units = units\r\n",
        "\r\n",
        "    self.cell_state = tf.zeros(shape=(BATCH_SIZE,1,3))\r\n",
        "\r\n",
        "    self.input_gate = tf.keras.layers.Dense(units=self.units, activation=SIGMOID) # filter with zeros and ones, shape the same as cell state (sigmoid)\r\n",
        "    # QUESTION: Forget gate bias soll auf 1 gesetzt werden, ich wei√ü nicht ob das so geht (weights = ...)\r\n",
        "    self.forget_gate = tf.keras.layers.Dense(units=self.units, activation=SIGMOID, weights=[_, tf.ones(self.units)]) # filter with zeros and ones, shape the same as cell state (sigmoid)\r\n",
        "    self.output_gate = tf.keras.layers.Dense(units=self.units, activation=SIGMOID) # applied to the (regularized) new cell state\r\n",
        "\r\n",
        "    self.cell_state_candidate = tf.keras.layers.Dense(units=self.units, activation=TANH) # matrix of shape of cell state, uses tanh\r\n",
        "    self.hidden_state = tf.zeros(shape=(BATCH_SIZE,1,3))# output x cell_state_candidate\r\n",
        "\r\n",
        "  def call(self, x, states):\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    x:        Input for a single timestep\r\n",
        "    states:   Tuple containing hidden state and cell state\r\n",
        "    \"\"\"\r\n",
        "    (hidden_state, cell_state) = states\r\n",
        "\r\n",
        "    # \r\n",
        "    concat_input = tf.concat([hidden_state, x])\r\n",
        "\r\n",
        "    # update cell state\r\n",
        "    self.cell_state = self.cell_state * self.forget_gate(concat_input)\r\n",
        "\r\n",
        "    update = self.input_gate(concat_input) * self.cell_state_candidate(concat_input)\r\n",
        "\r\n",
        "    self.cell_state = self.cell_state + update\r\n",
        "\r\n",
        "    # Output is the new hidden state\r\n",
        "    self.hidden_state = tf.nn.tanh(self.cell_state) * self.output_gate(concat_input)\r\n",
        "    \r\n",
        "    return self.hidden_state, self.cell_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehmO4XEhc0Wy"
      },
      "source": [
        "class LSTM(Model):\r\n",
        "\r\n",
        "  def __init__(self, units):\r\n",
        "    \"\"\"\r\n",
        "    \"\"\"\r\n",
        "    super(LSTM, self).__init__()\r\n",
        "\r\n",
        "    # We need a LSTM cell\r\n",
        "    self.lstm_cell = LSTM_Cell(units)\r\n",
        "\r\n",
        "\r\n",
        "  def call(self, x):\r\n",
        "    \"\"\"\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # A LSTM cell is called with data consisting of a target and a sequence\r\n",
        "    target, sequence = x[0], x[1]\r\n",
        "\r\n",
        "    # Length of sequence == timesteps ???\r\n",
        "    timesteps = len(sequence)\r\n",
        "\r\n",
        "    # For each timestep (digit in sequence) update the LSTM cell\r\n",
        "    for timestep in range(timesteps):\r\n",
        "\r\n",
        "      # Network input is the concatenation of target and each element in sequence\r\n",
        "      network_input = tf.concat([[sequence[timestep], target]], axis=0) # sequence[timestep] must be in square brackets to avoid shape issues\r\n",
        "      \r\n",
        "      # Update lstm cells cell state by calling the function\r\n",
        "      self.lstm_cell(network_input, (self.lstm_cell.hidden_state, self.lstm_cell.cell_state))\r\n",
        "\r\n",
        "    output = self.lstm_cell.hidden_state\r\n",
        "\r\n",
        "    self.lstm_cell.hidden_state = tf.zeros(shape=(BATCH_SIZE,1,3))\r\n",
        "    self.lstm_cell.cell_state = tf.zeros(shape=(BATCH_SIZE,1,3))\r\n",
        "\r\n",
        "    # Last hidden state is the output\r\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y21ufSjAyRS-"
      },
      "source": [
        "class Model(Model):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    \"\"\"\r\n",
        "    The model consists of an input layer, a LSTM layer and an output layer.\r\n",
        "    \"\"\"\r\n",
        "    super(Model, self).__init__()\r\n",
        "\r\n",
        "    # Input layer\r\n",
        "    #self.read_in = \r\n",
        "\r\n",
        "    # Calls a LSTM\r\n",
        "    #self.lstm = LSTM(10)\r\n",
        "\r\n",
        "    # Output layer\r\n",
        "    # Readout is a dense layer with three units (one hot encoding)\r\n",
        "    self.read_out = tf.keras.layers.Dense(units=3, activation=SIGMOID)\r\n",
        "\r\n",
        "\r\n",
        "  def call(self):\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "    # Calls LSTM with zero input (how?)\r\n",
        "    self.lstm = LSTM(10)\r\n",
        "    \r\n",
        "    # LSTM must be called with tuple: (target, sequence)\r\n",
        "    output = self.read_out(self.lstm())\r\n",
        "\r\n",
        "    return output\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT8UxXIFdo6b"
      },
      "source": [
        "lstm_model = Model()\r\n",
        "print(lstm_model.units)\r\n",
        "\r\n",
        "# for (target, seq, label) in train_data.take(1):\r\n",
        "#   lstm_model((target, seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmboszG1JHLc"
      },
      "source": [
        "This is where we got stuck and did not know what to do anymore. We went to the coding support session but we still did not really understand the relevance and task of the three different classes - for the LSTM_cell we are sure that it somehow is what it is supposed to be, but the distinction between the LSTM class and Model class is our problem here. Reason being: for the next step, we do not really understand, what will be the call/ task for our network in the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPCrnKPtsHNc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}