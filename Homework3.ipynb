{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "crOefMWFeqgg"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Layer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNOZDymJm-Kq"
      },
      "source": [
        "# Task 1: Data Set\n",
        "\n",
        "The following cell loads the first 10% of training and 1% of test examples of the 'genomics_ood' tensorflow dataset. The parameters in the tfds.load() function are shortly described, or more info please see the tensorflow overview on how to load datasets with tfds: https://www.tensorflow.org/datasets/overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyt-nZ4ceybi"
      },
      "source": [
        "# TODO: CHANGE TO 10% FOR TRAIN DATA TO GET 100.000 TRAINING EXAMPLES!!!\n",
        "\n",
        "(train_data, test_data), ds_info = tfds.load('genomics_ood', \n",
        "                                   split=['train[:1%]', 'test[:1%]'], # Only load the first 10% of train and 1% of the test examples of the dataset as we only want 100.000 train and 1.000 test examples\n",
        "                                   as_supervised=True, # This allows to call data, label for train and test data\n",
        "                                   shuffle_files=True, # Shuffle\n",
        "                                   try_gcs=True, # Load data from GCS bucket\n",
        "                                   download=False, # Do not download the dataset locally\n",
        "                                   with_info=True # This is only necessary if one wants to load info on the dataset (to variable ds_info)\n",
        "                                   )"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bhgxDoRnF0t"
      },
      "source": [
        "When loading the data set, one can choose to also get info on the dataset by setting with_info=True in tfds.load(). To take a look at the info, run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZHAsxKclKWm",
        "outputId": "d78eb88a-3a21-483b-ca6e-8afb29ad8ee0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print general info on dataset\n",
        "print(ds_info)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='genomics_ood',\n",
            "    version=0.0.1,\n",
            "    description='Bacteria identification based on genomic sequences holds the promise of early\n",
            "detection of diseases, but requires a model that can output low confidence\n",
            "predictions on out-of-distribution (OOD) genomic sequences from new bacteria\n",
            "that were not present in the training data.\n",
            "\n",
            "We introduce a genomics dataset for OOD detection that allows other researchers\n",
            "to benchmark progress on this important problem. New bacterial classes are\n",
            "gradually discovered over the years. Grouping classes by years is a natural way\n",
            "to mimic the in-distribution and OOD examples.\n",
            "\n",
            "The dataset contains genomic sequences sampled from 10 bacteria classes that\n",
            "were discovered before the year 2011 as in-distribution classes, 60 bacteria\n",
            "classes discovered between 2011-2016 as OOD for validation, and another 60\n",
            "different bacteria classes discovered after 2016 as OOD for test, in total 130\n",
            "bacteria classes. Note that training, validation, and test data are provided for\n",
            "the in-distribution classes, and validation and test data are proviede for OOD\n",
            "classes. By its nature, OOD data is not available at the training time.\n",
            "\n",
            "The genomic sequence is 250 long, composed by characters of {A, C, G, T}. The\n",
            "sample size of each class is 100,000 in the training and 10,000 for the\n",
            "validation and test sets.\n",
            "\n",
            "For each example, the features include:\n",
            "  seq: the input DNA sequence composed by {A, C, G, T}.\n",
            "  label: the name of the bacteria class.\n",
            "  seq_info: the source of the DNA sequence, i.e., the genome name, NCBI\n",
            "  accession number, and the position where it was sampled from.\n",
            "  domain: if the bacteria is in-distribution (in), or OOD (ood)\n",
            "\n",
            "The details of the dataset can be found in the paper supplemental.',\n",
            "    homepage='https://github.com/google-research/google-research/tree/master/genomics_ood',\n",
            "    features=FeaturesDict({\n",
            "        'domain': Text(shape=(), dtype=tf.string),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=130),\n",
            "        'seq': Text(shape=(), dtype=tf.string),\n",
            "        'seq_info': Text(shape=(), dtype=tf.string),\n",
            "    }),\n",
            "    total_num_examples=2400000,\n",
            "    splits={\n",
            "        'test': 100000,\n",
            "        'test_ood': 600000,\n",
            "        'train': 1000000,\n",
            "        'validation': 100000,\n",
            "        'validation_ood': 600000,\n",
            "    },\n",
            "    supervised_keys=('seq', 'label'),\n",
            "    citation=\"\"\"@inproceedings{ren2019likelihood,\n",
            "      title={Likelihood ratios for out-of-distribution detection},\n",
            "      author={Ren, Jie and\n",
            "      Liu, Peter J and\n",
            "      Fertig, Emily and\n",
            "      Snoek, Jasper and\n",
            "      Poplin, Ryan and\n",
            "      Depristo, Mark and\n",
            "      Dillon, Joshua and\n",
            "      Lakshminarayanan, Balaji},\n",
            "      booktitle={Advances in Neural Information Processing Systems},\n",
            "      pages={14707--14718},\n",
            "      year={2019}\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjq9jxCgng_-"
      },
      "source": [
        "Also, we can take a look at the data. Because we set as_supervised=True when loading the data, we can iterate over either train or test data by calling 'for data, label in train:' and print the info we want. In the following cell, the first 5 training examples and labels will be printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3636jPU6fim7",
        "outputId": "383fc770-9a60-4a1d-d90f-cd646494dadc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# If one wants to take a look at the data\n",
        "for i, (input, label) in enumerate(train_data): # This is possible because we set as_supervised=True when loading the dataset\n",
        "  print('(' + str(i+1) + ')')\n",
        "\n",
        "  # Data is a of type string:\n",
        "  tf.print('Input:', input) # Use tf.print to only print string element (for print(data), the entire tensor content will be printed)\n",
        "\n",
        "  # Labels are int ranging from 0-9\n",
        "  tf.print('Label:', label, '\\n')\n",
        "\n",
        "  # This is to interrupt after 5 examples where shown\n",
        "  if(i >= 4):\n",
        "    break"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1)\n",
            "Input: \"CACAGCCGGCCGCTGACCTGCTGGCCATCGGAGCGCTGGCCGGCCTCGAGGATATTGGCCAGCAGCAGGTGGATGTCTCGGGCATAGCGCTCCCCCTGGTAGGTGATGCGAATGCTGCGGCCCTGGCGCTCGGTCAGGGCAAAACCCAGGGTCTGCTCCAGGCTCTTGATCTGGTGGCTGATGGCGCTGGGCGTCAGGTTCAGCTCATTGGCGGCCTCGGCGACACTGCCCAGGCGGGCCACCGCGTCCA\"\n",
            "Label: 5 \n",
            "\n",
            "(2)\n",
            "Input: \"AAACTATGTTATATTCACGATGATTAACTTACAAAGGAGTTTCAACTATGAAGATGATAAACAAATTAATCGTTCCGGTAACAGCTAGTGCTTTATTATTAGGCGCTTGTGGCGCTAGTGCCACAGACTCTAAAGAAAATACATTAATTTCTTCTAAAGCTGGAGACGTAACAGTTGCAGATACAATGAAAAAAATCGGTAAAGATCAAATTGCAAATGCATCATTTACTGAAATGTTAAATAAAATTTT\"\n",
            "Label: 7 \n",
            "\n",
            "(3)\n",
            "Input: \"CGCCGGCACCGTTGCTGGCCAAAATCGCCGAGCGTCCGGATGCCGGCATGCATCGTGAATCGTCTTATCTGAAATGGCACTGGCGCGTTTGCCGGGAACTTCTCCAACGTCGGGAGCACGGGGCAACTCATGGCTAAACTCATCGTGGGCAACGTCGATAACGAAGCAATGATCGGGGACACGAAGCGTGCATCGCTTCCGCTTCGCCAGGTATCAGCGATTGCAGCAAGGCGCCTCGTCTGGCAGATGA\"\n",
            "Label: 1 \n",
            "\n",
            "(4)\n",
            "Input: \"GCAGGTGCTGTTGGCCGGCACCAACCACCACATCCGCCTGCTGCAGAATGGCCAGCTGGCCTACACTGCCGAGCCGGTCAACGAAATCTATCGGCCTTCGATCGATGTGTTCTTCGAAAGCGTCGCGCGCTATTGGTCGGGCGATGCGGTGGGCGTGCTGCTCACCGGCATGGGCCGCGATGGTGCCCAGGGCCTGAAGCTGATGCGCCAGCAGGGCTTCCTGACCATCGCCCAGGACCAGCAGAGCAGC\"\n",
            "Label: 5 \n",
            "\n",
            "(5)\n",
            "Input: \"TCCGGACGCCAATTTATTACCGGTAAGAAAAGCGAACTCACCTAATTTTACGGTCTGGTGACCAGAACCGACCGCACGCAAAAGCTTCTGAACCGATGAGGATGCTATGGGAAACACAACAATACAAACGCAGAGTTTTCGTGCTGTGGATGCAGAGCAAAGCAAGAGCAAGCGCTACATTATTCCATTCGCCTTACTTTGCTCGCTATTTTTTCTGTGGGCTGTCGCCAATAATCTGAATGACATTTTA\"\n",
            "Label: 6 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ITQtOemr-dt"
      },
      "source": [
        "The goal is to have the data one-hot-encoded. For this, we can first use the map function calling a function that encodes the letters to digits from 0-3, then constructs onehot encoding for each of the three numbers (0-3) and then applies this encoding to the previously 'translated' string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYlsYP5lGxLk"
      },
      "source": [
        "# Mapping dictionary from letters to numbers\n",
        "mapping = {'A': '0', 'C': '1', 'G': '2', 'T': '3'}\n",
        "\n",
        "def one_hot_encoding(input, label):\n",
        "  \"\"\"\n",
        "  Preprocess inputs and labels: Create onehot encoded input and labelling.\n",
        "  \"\"\"\n",
        "\n",
        "  # Encode each letter to one number 0-3 according to 'mapping'-dictionary\n",
        "  for key in mapping:\n",
        "    input = tf.strings.regex_replace(input, key, mapping[key])\n",
        "  \n",
        "  # Split after each number\n",
        "  split = tf.strings.bytes_split(input)\n",
        "\n",
        "  # Encode input to Onehot encoding (numbers are strings atm, so convert those \n",
        "  # to ints before onehot encoding them)\n",
        "  l = tf.cast(tf.strings.to_number(split), tf.uint8)\n",
        "  onehot_input = tf.one_hot(l, 4)\n",
        "  onehot_input = tf.reshape(onehot_input, (-1,))\n",
        "\n",
        "  # Encode labels to onehot encoding\n",
        "  onehot_label = tf.one_hot(label, 10)\n",
        "  onehot_label = tf.reshape(onehot_label, (-1,10))\n",
        "\n",
        "  return [onehot_input], onehot_label"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p40q9I_hYlfX",
        "outputId": "838893ed-f28b-400d-9548-a9246055c698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_data_new = tf.data.Dataset.from_tensors(train_data)\n",
        "t = train_data_new.take(1)\n",
        "\n",
        "for input in t:\n",
        "  print(input)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_VariantDataset shapes: ((), ()), types: (tf.string, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpahIJUpr2Rv"
      },
      "source": [
        "# Preprocessing pipeline for train and test data\n",
        "train_data_encoded = train_data.map(one_hot_encoding)\n",
        "test_data_encoded = test_data.map(one_hot_encoding)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIJ3RhFMsWu4",
        "outputId": "8c911ee6-9db1-4b20-fa48-9066d84cc211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: this can be deleted, only to take a look \n",
        "this = train_data_encoded.take(1)\n",
        "\n",
        "for input, label in this:\n",
        "  print(input)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            "  1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
            "  0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
            "  1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.\n",
            "  0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            "  1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
            "  0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
            "  0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
            "  1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            "  1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            "  1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.\n",
            "  0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            "  1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
            "  0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
            "  1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            "  1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
            "  1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            "  1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            "  1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.]], shape=(1, 1000), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQK3yBTNt5ws"
      },
      "source": [
        "# Task 2: Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHCFE6abs0rx"
      },
      "source": [
        "class Model(Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    # Call super-class (of Model)\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    # Get activation functions\n",
        "    SIGMOID = tf.keras.activations.sigmoid\n",
        "    SOFTMAX = tf.keras.activations.softmax\n",
        "\n",
        "    # Define fully connected (Dense) layers: two hidden layers with 256 units each, one output layer\n",
        "    self.hidden_layer_1 = tf.keras.layers.Dense(256, activation=SIGMOID)\n",
        "    self.hidden_layer_2 = tf.keras.layers.Dense(256, activation=SIGMOID)\n",
        "\n",
        "    # Using softmax as output for network because we have 10 categories\n",
        "    self.output_layer = tf.keras.layers.Dense(10, activation=SOFTMAX)\n",
        "\n",
        "  \n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    # Define the forward step through two hidden layers to output layer\n",
        "    x = self.hidden_layer_1(x)\n",
        "    x = self.hidden_layer_2(x)\n",
        "    x = self.output_layer(x)\n",
        "    return x\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlUux8wxwVIW"
      },
      "source": [
        "# Task 3: Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnhmhshEzELw"
      },
      "source": [
        "# Define parameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "running_average_factor = 0.95\n",
        "LOSS = tf.keras.losses.categorical_crossentropy\n",
        "OPTIMIZER = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "# Initialize model\n",
        "model = Model()\n",
        "\n",
        "# Lists to store losses and accuracies for training and test steps\n",
        "train_losses, train_accuracies = [], []\n",
        "test_losses, test_accuracies = [], []"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKY5UfTOwL84"
      },
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "  \"\"\"\n",
        "  One training step on a given model with specified input, target, loss function \n",
        "  and optimizer.\n",
        "  \"\"\"\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(input)\n",
        "    loss = loss_function(target, prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "  \"\"\"\n",
        "  Tests model on test data to receive loss and accuracy on previously unseen \n",
        "  data.\n",
        "  \"\"\"\n",
        "\n",
        "  test_accuracy_aggregator = []\n",
        "  test_loss_aggregator = []\n",
        "\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "    sample_test_loss = loss_function(target, prediction)\n",
        "    sample_test_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
        "\n",
        "    test_loss = np.mean(test_loss_aggregator)\n",
        "    test_accuracy = np.mean(test_accuracy_aggregator)\n",
        "\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNjWt-dsylmA",
        "outputId": "2234fbe7-beda-4169-deee-9ac9ad6a937c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "import time\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  start = time.time()\n",
        "  print('Start epoch ' + str(epoch) + '...')\n",
        "\n",
        "  running_average = 0\n",
        "\n",
        "  for i, (input, target) in enumerate(train_data_encoded):\n",
        "    train_loss = train_step(model, input, target, LOSS, OPTIMIZER)\n",
        "    running_average = running_average_factor * running_average + (1 - running_average_factor) * train_loss\n",
        "  \n",
        "  train_losses.append(running_average)\n",
        "\n",
        "  test_loss, test_accuracy = test(model, test_data_encoded, LOSS)\n",
        "  test_losses.append(test_loss)\n",
        "  test_accuracies.append(test_accuracy)\n",
        "\n",
        "  # Measure time required for each epoch\n",
        "  print('... finished after ' + str(time.time() - start) + ' seconds')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start epoch 0...\n",
            "... finished after 67.1598310470581 seconds\n",
            "Start epoch 1...\n",
            "... finished after 65.94725322723389 seconds\n",
            "Start epoch 2...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-dbc98e638e6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOSS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mrunning_average\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_average_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrunning_average\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrunning_average_factor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-0ace8b7d1894>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, input, target, loss_function, optimizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    547\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m           kwargs={\n\u001b[0;32m--> 549\u001b[0;31m               \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m           })\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m     merge_fn = autograph.tf_convert(\n\u001b[1;32m   2714\u001b[0m         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 2715\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2720\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   2721\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2722\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2723\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign_add\u001b[0;34m(self, delta, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    825\u001b[0m           name=name)\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_add_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0massign_add_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_lazy_read\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m     \u001b[0mvariable_accessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m     return _UnreadVariable(\n\u001b[1;32m    833\u001b[0m         \u001b[0mhandle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mvariable_accessed\u001b[0;34m(variable)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvariable_accessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;34m\"\"\"Records that `variable` was accessed for the tape and FuncGraph.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"watch_variable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m   6003\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6004\u001b[0m   \"\"\"\n\u001b[0;32m-> 6005\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_default_graph_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhyAJBLj0Dl6",
        "outputId": "ce749941-f368-468f-e7f2-d821b50af442",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_accuracies"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-92Aktnuh2Om"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}